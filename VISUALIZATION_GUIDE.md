# Data Visualization Techniques Used in EPL Prediction Project

## Quick Answer for Defense

**"What visualization techniques did you use?"**

**Answer:** "I used **three main visualization techniques**: 
1. **Confusion Matrix Heatmaps** using Seaborn to visualize model prediction accuracy
2. **Classification Report Bar Charts** using Matplotlib to compare precision, recall, and F1-scores across classes
3. **Pandas Cross-tabulation Tables** for numeric comparison of predictions vs actuals

I also used **pandas DataFrames** for tabular data exploration and **percentage matrices** to show normalized confusion results."

---

## üìä DETAILED BREAKDOWN OF VISUALIZATIONS

### 1. **CONFUSION MATRIX HEATMAP (Seaborn)**

#### **Code Used:**
```python
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

cm = confusion_matrix(combined["actual"], combined["prediction"])
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", 
            xticklabels=[0,1], yticklabels=[0,1])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()
```

#### **Purpose:**
- Visualizes how well predictions match actual outcomes
- Shows True Positives, False Positives, True Negatives, False Negatives
- Color intensity indicates frequency

#### **Visualization Details:**
- **Library:** Seaborn (`sns.heatmap`)
- **Type:** Heatmap
- **Color Scheme:** "Blues" (darker blue = more frequent)
- **Annotations:** `annot=True, fmt="d"` shows actual numbers in each cell
- **Axes Labels:** Predicted (X-axis) vs Actual (Y-axis)

#### **What It Shows:**
- **Binary Model (Home Win vs Not Win):**
  - Diagonal cells = Correct predictions
  - Off-diagonal = Misclassifications
  
- **Multi-class Model (Win/Draw/Loss):**
  - 3√ó3 matrix showing all prediction combinations
  - Labels: "Home Win", "Draw", "Away Win"

---

### 2. **CONFUSION MATRIX DISPLAY (Scikit-learn)**

#### **Code Used:**
```python
from sklearn.metrics import ConfusionMatrixDisplay

cm = confusion_matrix(multiclass_test["result_label"], multi_preds)
disp = ConfusionMatrixDisplay(
    confusion_matrix=cm,
    display_labels=["Home Win", "Draw", "Away Win"]
)
disp.plot(cmap="Blues")
plt.title("Multiclass outcome confusion matrix")
plt.show()
```

#### **Purpose:**
- Professional-looking confusion matrix with proper labeling
- Better for multi-class problems

#### **Visualization Details:**
- **Library:** Scikit-learn's `ConfusionMatrixDisplay`
- **Advantage:** Automatically formats labels and axes
- **Color Map:** "Blues"
- **Custom Labels:** "Home Win", "Draw", "Away Win" instead of just 0, 1, 2

---

### 3. **PERCENTAGE CONFUSION MATRIX (Pandas DataFrame)**

#### **Code Used:**
```python
cm = confusion_matrix(multiclass_test["result_label"], multi_preds)
cm_percent = (cm / cm.sum(axis=1, keepdims=True) * 100).round(1)

pd.DataFrame(
    cm_percent,
    columns=["Pred Home", "Pred Draw", "Pred Away"],
    index=["Actual Home", "Actual Draw", "Actual Away"]
)
```

#### **Purpose:**
- Shows **percentage** of predictions per actual class
- Easier to understand class-wise accuracy than raw numbers

#### **Example Output:**
```
                Pred Home  Pred Draw  Pred Away
Actual Home          71.0        8.5       20.5
Actual Draw          60.0       12.4       27.6
Actual Away          41.9        9.8       48.3
```

**Interpretation:** Of all actual home wins, 71% were predicted correctly, 8.5% predicted as draws, 20.5% as away wins.

---

### 4. **CROSS-TABULATION (Pandas)**

#### **Code Used:**
```python
combined = pd.DataFrame(dict(actual=test["target"], prediction=preds))
pd.crosstab(index=combined["actual"], columns=combined["prediction"])
```

#### **Purpose:**
- Simple numeric table showing prediction counts
- Quick overview without graphics

#### **Example Output:**
```
prediction     0    1
actual               
0           1328  342
1            744  588
```

---

### 5. **BAR CHART FOR CLASSIFICATION METRICS (Matplotlib)**

#### **Code Used:**
```python
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Generate classification report
target_names = ["Home Win", "Draw", "Away Win"]
report = classification_report(
    multiclass_test["result_label"], 
    multi_preds, 
    target_names=target_names, 
    output_dict=True
)
df_report = pd.DataFrame(report).transpose()

# Extract per-class metrics
metrics_df = df_report.loc[target_names, ['precision', 'recall', 'f1-score']]

# Bar chart
plt.figure(figsize=(10, 6))
metrics_df.plot(kind='bar', ax=plt.gca())
plt.title("Per-Class Performance Metrics", fontsize=14)
plt.ylabel("Score", fontsize=12)
plt.xlabel("Class", fontsize=12)
plt.xticks(rotation=0)
plt.ylim(0, 1.0)
plt.legend(loc='lower right', title="Metric")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()
```

#### **Purpose:**
- Compare precision, recall, and F1-score across all three classes
- Easy visual comparison of which classes are harder to predict

#### **Visualization Details:**
- **Type:** Grouped bar chart
- **X-axis:** Classes (Home Win, Draw, Away Win)
- **Y-axis:** Metric scores (0.0 to 1.0)
- **Colors:** Different color for each metric
- **Grid:** Horizontal gridlines for easier reading

---

### 6. **METRICS HEATMAP (Seaborn)**

#### **Code Used:**
```python
plt.figure(figsize=(8, 5))
sns.heatmap(metrics_df, annot=True, cmap="YlGnBu", fmt=".2f", linewidths=.5)
plt.title("Performance Metrics Heatmap", fontsize=14)
plt.tight_layout()
plt.show()
```

#### **Purpose:**
- Compact view of all metrics for all classes
- Color coding shows performance intensity

#### **Visualization Details:**
- **Library:** Seaborn heatmap
- **Color Map:** "YlGnBu" (Yellow-Green-Blue)
- **Format:** 2 decimal places (`.2f`)
- **Annotations:** Shows exact values
- **Line width:** Separates cells clearly

---

### 7. **PREDICTION RESULTS TABLE (Pandas DataFrame)**

#### **Code Used:**
```python
results_df = pd.DataFrame({
    'Date': multiclass_test['Date'],
    'HomeTeam': multiclass_test['HomeTeam'],
    'AwayTeam': multiclass_test['AwayTeam'],
    'result_label': multiclass_test['result_label'],
    'home_win_pct': proba[:, 0] * 100,
    'draw_pct': proba[:, 1] * 100,
    'away_win_pct': proba[:, 2] * 100,
    'predicted_outcome': predicted_outcomes,
    'actual_outcome': actual_outcomes
})
display(results_df.tail())
```

#### **Purpose:**
- Show actual probability predictions for recent matches
- Verify model is giving sensible probabilities

#### **Example Output:**
```
        Date     HomeTeam     AwayTeam  home_win_pct  draw_pct  away_win_pct
0 2025-11-09  Crystal Palace  Brighton         66.8     20.4         12.8
1 2025-11-09     Man City     Liverpool        41.3     36.3         22.3
```

---

## üé® VISUALIZATION LIBRARIES USED

### **Primary Libraries:**

1. **Matplotlib** (`matplotlib.pyplot as plt`)
   - Foundational plotting library
   - Used for: Figure creation, titles, labels, layout control
   
2. **Seaborn** (`import seaborn as sns`)
   - High-level visualization built on matplotlib
   - Used for: Heatmaps with better aesthetics
   
3. **Pandas** (`pd.DataFrame`, `pd.crosstab`)
   - Data manipulation and display
   - Used for: Tabular data visualization, cross-tabulation
   
4. **Scikit-learn** (`ConfusionMatrixDisplay`)
   - ML-specific visualization tools
   - Used for: Professional confusion matrices

---

## üìà TYPES OF VISUALIZATIONS BY PURPOSE

### **For Model Performance:**
| Visualization | Purpose | Library |
|---------------|---------|---------|
| Confusion Matrix Heatmap | Overall accuracy view | Seaborn |
| Percentage Confusion Matrix | Class-wise accuracy | Pandas + calculation |
| Classification Report | Precision/Recall/F1 | Scikit-learn (text) |
| Metrics Bar Chart | Compare metrics visually | Matplotlib |
| Metrics Heatmap | Compact metric overview | Seaborn |

### **For Data Exploration:**
| Visualization | Purpose | Library |
|---------------|---------|---------|
| DataFrame `.head()` | Quick data preview | Pandas |
| Cross-tabulation | Simple count comparison | Pandas |
| Value counts table | Class distribution | Pandas |

### **For Predictions:**
| Visualization | Purpose | Library |
|---------------|---------|---------|
| Probability table | Show model confidence | Pandas |
| Results DataFrame | Match-by-match predictions | Pandas |

---

## üéØ DEFENSE QUESTIONS & ANSWERS

### **Q1: "Why did you use heatmaps for confusion matrices?"**
**A:** "Heatmaps provide an intuitive visual representation where color intensity immediately shows which prediction combinations are most common. This makes it easier to identify patterns of misclassification at a glance compared to just looking at numbers."

### **Q2: "What's the difference between your confusion matrix visualizations?"**
**A:** "I used two approaches:
1. **Seaborn heatmap** - Quick, aesthetic visualization for binary classification
2. **ConfusionMatrixDisplay** - More formal scikit-learn method for multi-class with proper labeling

Both show the same information but ConfusionMatrixDisplay is better for presentations with its cleaner labels."

### **Q3: "Why use both tables and visualizations?"**
**A:** "Tables (like crosstab) give exact numbers for detailed analysis, while visualizations (like heatmaps) provide quick pattern recognition. Together they offer both precision and intuition."

### **Q4: "Did you create any plots for the data itself, not just results?"**
**A:** "My focus was on model evaluation visualizations. For data exploration, I used pandas DataFrames and value_counts tables rather than plots, which is appropriate for tabular match statistics."

### **Q5: "What color schemes did you use and why?"**
**A:** "I consistently used the **'Blues' color map** for confusion matrices because:
- Professional appearance
- Color blind friendly
- Darker = more frequent (intuitive)
- Matches the positive/performance connotation

For metrics heatmap, I used **'YlGnBu' (Yellow-Green-Blue)** for better differentiation across the 0-1 scale."

### **Q6: "How do you interpret the confusion matrix?"**
**A:** "The diagonal represents **correct predictions**. Off-diagonal cells show **misclassifications**. For example, if row 0, column 1 has value 342, it means 342 actual 'No Wins' were incorrectly predicted as 'Win'."

---

## üí° KEY VISUALIZATION TECHNIQUES SUMMARY

### **What You Should Say:**

1. **Primary Technique:** "Confusion Matrix Heatmap using Seaborn"
   
2. **Why This Technique:** "Visual clarity for multi-class classification results"
   
3. **Supporting Techniques:**
   - Bar charts for metric comparison
   - Percentage matrices for normalized results
   - DataFrames for tabular display

4. **Libraries Used:**
   - Matplotlib (plotting foundation)
   - Seaborn (aesthetic heatmaps)
   - Pandas (tabular visualization)
   - Scikit-learn (specialized ML plots)

---

## üîç CODE SNIPPETS FOR QUICK REFERENCE

### **Minimal Confusion Matrix:**
```python
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.show()
```

### **With Labels:**
```python
from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay(
    confusion_matrix=cm,
    display_labels=["Win", "Draw", "Loss"]
).plot(cmap='Blues')
plt.show()
```

---

## ‚úÖ WHAT YOU DID NOT USE (Be Honest)

**Techniques NOT used in your project:**
- ‚ùå Line plots (for trends over time)
- ‚ùå Scatter plots (for feature relationships)
- ‚ùå Histograms or distribution plots
- ‚ùå ROC curves or AUC plots
- ‚ùå Feature importance plots
- ‚ùå Box plots or violin plots

**Why you didn't use them:**
- Focus was on classification performance, not feature analysis
- For defense, you can say: *"I prioritized visualizations directly related to model performance evaluation. Feature importance analysis would be valuable future work."*

---

## üéì CONFIDENCE STATEMENT FOR DEFENSE

**When explaining your visualizations:**

"I employed **confusion matrix heatmaps** as my primary visualization technique because they provide immediate visual feedback on classification performance. Using **Seaborn's heatmap function**, I created color-coded matrices where the diagonal represents correct predictions and off-diagonal cells reveal misclassification patterns.

I supplemented this with **percentage-normalized matrices** to understand class-wise accuracy, and **bar charts** to compare precision, recall, and F1-scores across the three outcome classes (Home Win, Draw, Away Win).

All visualizations used the **Blues color scheme** for consistency and professional presentation, and I leveraged **Matplotlib** for figure control and **Scikit-learn's ConfusionMatrixDisplay** for publication-quality outputs."

---

## üìö YOU'RE READY!

**Key Points to Remember:**
1. Primary visualization = **Confusion Matrix Heatmap** (Seaborn)
2. Secondary = **Bar charts** for metrics, **Tables** for numbers
3. Libraries = **Matplotlib + Seaborn + Pandas + Scikit-learn**
4. Purpose = **Evaluate model performance, not explore data**
5. Color scheme = **Blues** (professional, intuitive)

**You've got this! üöÄ**
